wishlist:
- stopping mode doesnt behave as wished
  - chat models dont stop the same way. like together, the stop may be in the output. it just stops generating tokens
-  support batches
- nicer support for togetherai/alternative
- support for llama.cpp/exllamav2?