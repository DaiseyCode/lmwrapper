{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p><code>lmwrapper</code> provides a wrapper around OpenAI API and Hugging Face Language models, focusing on being a clean, object-oriented, and user-friendly interface. It has two main goals:</p> <p>A) Make it easier to use the OpenAI API.</p> <p>B) Make it easier to reuse your code for other language models with minimal changes.</p> <p>Some key features currently include local caching of responses, and super simple use of the OpenAI batching API which can save 50% on costs.</p> <p><code>lmwrapper</code> is lightweight and can serve as a flexible stand-in for the OpenAI API.</p>"},{"location":"#installation","title":"Installation","text":"<p>For usage with just OpenAI models:</p> <pre><code>pip install lmwrapper\n</code></pre> <p>For usage with HuggingFace models as well:</p> <pre><code>pip install 'lmwrapper[hf]'\n</code></pre> <p>For development dependencies:</p> <pre><code>pip install 'lmwrapper[dev]'\n</code></pre>"},{"location":"#example-usage","title":"Example usage","text":""},{"location":"#chat","title":"Chat","text":"<pre><code>from lmwrapper.openai_wrapper import get_open_ai_lm, OpenAiModelNames\nfrom lmwrapper.structs import LmPrompt, LmChatTurn\n\nlm = get_open_ai_lm(\n    model_name=OpenAiModelNames.gpt_4o_mini,\n    api_key_secret=None,  # By default, this will read from the OPENAI_API_KEY environment variable.\n    # If that isn't set, it will try the file ~/oai_key.txt\n    # You need to place the key in one of these places,\n    # or pass in a different location. You can get an API\n    # key at (https://platform.openai.com/account/api-keys)\n)\n\n# Single user utterance\npred = lm.predict(\"What is 2+2?\")\nprint(pred.completion_text)  # \"2 + 2 equals 4.\"\n\n\n# Use a LmPrompt to have more control of the parameters\npred = lm.predict(LmPrompt(\n    \"What is 2+6?\",\n    max_tokens=10,\n    temperature=0, # Set this to 0 for deterministic completions\n))\nprint(pred.completion_text)  # \"2 + 6 equals 8.\"\n\n# Conversation alternating between `user` and `assistant`.\npred = lm.predict(LmPrompt(\n    [\n        \"What is 2+2?\",  # user turn\n        \"4\",  # assistant turn\n        \"What is 5+3?\"  # user turn\n        \"8\",  # assistant turn\n        \"What is 4+4?\"  # user turn\n        # We use few-shot turns to encourage the answer to be our desired format.\n        #   If you don't give example turns you might get something like\n        #   \"4 + 4 equals 8.\" instead of just \"8\" as desired.\n    ],\n    max_tokens=10,\n))\nprint(pred.completion_text)  # \"8\"\n\n# If you want things like the system message, you can use LmChatTurn objects\npred = lm.predict(LmPrompt(\n    text=[\n        LmChatTurn(role=\"system\", content=\"You always answer like a pirate\"),\n        LmChatTurn(role=\"user\", content=\"How does bitcoin work?\"),\n    ],\n    max_tokens=25,\n    temperature=0,\n))\nprint(pred.completion_text)\n# \"Arr, me matey! Bitcoin be a digital currency that be workin' on a technology called blockchain...\"\n</code></pre>"},{"location":"#caching","title":"Caching","text":"<p>Add <code>caching = True</code> in the prompt to cache the output to disk. Any subsequent calls with this prompt will return the same value. Note that this might be unexpected behavior if your temperature is non-zero. (You will always sample the same output on reruns). If you want to get multiple samples at a non-zero temperature while still using the cache, you  set <code>num_completions &gt; 1</code> in a <code>LmPrompt</code>.</p>"},{"location":"#openai-batching","title":"OpenAI Batching","text":"<p>The OpenAI batching API has a 50% reduced cost when willing to accept a 24-hour turnaround. This makes it good for processing datasets or other non-interactive tasks (which is the main target for <code>lmwrapper</code> currently).</p> <p><code>lmwrapper</code> takes care of managing the batch files and other details so that it's as easy  as the normal API.</p> <pre><code>from lmwrapper.openai_wrapper import get_open_ai_lm, OpenAiModelNames\nfrom lmwrapper.structs import LmPrompt\nfrom lmwrapper.batch_config import CompletionWindow\n\ndef load_dataset() -&gt; list:\n    \"\"\"Load some toy task\"\"\"\n    return [\"France\", \"United States\", \"China\"]\n\ndef make_prompts(data) -&gt; list[LmPrompt]:\n    \"\"\"Make some toy prompts for our data\"\"\"\n    return [\n        LmPrompt(\n            f\"What is the capital of {country}? Answer with just the city name.\",\n            max_tokens=10,\n            temperature=0,\n            cache=True,\n        ) \n        for country in data\n    ]\n\ndata = load_dataset()\nprompts = make_prompts(data)\nlm = get_open_ai_lm(OpenAiModelNames.gpt_3_5_turbo)\npredictions = lm.predict_many(\n    prompts,\n    completion_window=CompletionWindow.BATCH_ANY \n    #                 ^ swap out for CompletionWindow.ASAP\n    #                   to complete as soon as possible via\n    #                   the non-batching API at a higher cost.\n) # The batch is submitted here\n\nfor ex, pred in zip(data, predictions):  # Will wait for the batch to complete\n    print(f\"Country: {ex} --- Capital: {pred.completion_text}\")\n    if ex == \"France\": assert pred.completion_text == \"Paris\" \n    # ...\n</code></pre> <p>The above code could technically take up to 24hrs to complete. However, OpenAI seems to complete these quicker (for example, these three prompts in ~1 minute or less). In a large batch, you don't have to keep the process running for hours. Thanks to <code>lmwrapper</code> cacheing it will automatically load or pick back up waiting on the existing batch when the script is reran.</p> <p>The <code>lmwrapper</code> cache lets you also intermix cached and uncached examples.</p> <pre><code># ... above code\n\ndef load_more_data() -&gt; list:\n    \"\"\"Load some toy task\"\"\"\n    return [\"Mexico\", \"Canada\"]\n\ndata = load_data() + load_more_data()\nprompts = make_prompts(data)\n# If we submit the five prompts, only the two new prompts will be\n# submitted to the batch. The already completed prompts will\n# be loaded near-instantly from the local cache.\npredictions = list(lm.predict_many(\n    prompts,\n    completion_window=CompletionWindow.BATCH_ANY\n))\n</code></pre> <p><code>lmwrapper</code> is designed to automatically manage the batching of thousands or millions of prompts.  If needed, it will automatically split up prompts into sub-batches and will manage issues around rate limits.</p> <p>This feature is mostly designed for the OpenAI cost savings. You could swap out the model for HuggingFace and the same code will still work. However, internally it is like a loop over the prompts. Eventually in <code>lmwrapper</code> we want to do more complex batching if GPU/CPU/accelerator memory is available.</p>"},{"location":"#caveats-implementation-needs","title":"Caveats / Implementation needs","text":"<p>This feature is still somewhat experimental. It likely works in typical usecases, but there are few known things  to sort out / TODOs:</p> <ul> <li>[X] Retry batch API connection errors</li> <li>[X] Automatically splitting up batches when have &gt;50,000 prompts (limit from OpenAI) </li> <li>[X] Recovering / splitting up batches when hitting your token Batch Queue Limit (see docs on limits)</li> <li>[X] Handle canceled batches during current run (use the web interface to cancel)</li> <li>[X] Handle/recover canceled batches outside of current run</li> <li>[X] Handle if openai batch expires unfinished in 24hrs (though not actually tested or observed this)</li> <li>[X] Automatically splitting up batch when exceeding 100MB prompts limit</li> <li>[X] Handling of failed prompts (like when have too many tokens). Use LmPrediction.has_errors and LmPrediction.error_message to check for an error on a response.</li> <li>[ ] Handle when there are duplicate prompts in batch submission</li> <li>[ ] Handle when a given prompt has <code>num_completions&gt;1</code></li> <li>[ ] Automatically clean up API files after done (right now end up with a lot of file in storage. There isn't an obvious cost for these batch files, but this might change and it would be better to clean them up.)</li> <li>[ ] Test on free-tier accounts. It is not clear what the tiny request limit counts</li> <li>[ ] Fancy batching of HF</li> <li>[ ] Concurrent batching when in ASAP mode</li> </ul> <p>Please open an issue if you want to discuss one of these or something else.</p> <p>Note, in the progress bars in PyCharm can be bit cleaner if you enable  terminal emulation in your run configuration.</p>"},{"location":"#hugging-face-models","title":"Hugging Face models","text":"<p>Local Causal LM models on Hugging Face models can be used interchangeably with the OpenAI models.</p> <p>Note: The universe of Huggingface models is diverse and inconsistent. Some (especially the non-completion ones) might require special prompt formatting to work as expected. Some models might not work at all.</p> <pre><code>from lmwrapper.huggingface_wrapper import get_huggingface_lm\nfrom lmwrapper.structs import LmPrompt\n\n# Download a small model for demo\nlm = get_huggingface_lm(\"gpt2\") # 124M parameters\n\nprediction = lm.predict(LmPrompt(\n    \"The capital of Germany is Berlin. The capital of France is\",\n    max_tokens=1,\n    temperature=0,\n))\nprint(prediction.completion_text)\nassert prediction.completion_text == \" Paris\"\n</code></pre> <p>Additionally, with HuggingFace models <code>lmwrapper</code> provides an interface for accessing the model internal states.</p>"},{"location":"#retries-on-rate-limit","title":"Retries on rate limit","text":"<pre><code>from lmwrapper.openai_wrapper import *\n\nlm = get_open_ai_lm(\n    OpenAiModelNames.gpt_3_5_turbo_instruct,\n    retry_on_rate_limit=True\n)\n</code></pre>"},{"location":"#other-features","title":"Other features","text":""},{"location":"#built-in-token-counting","title":"Built-in token counting","text":"<pre><code>from lmwrapper.openai_wrapper import *\nfrom lmwrapper.structs import LmPrompt\n\nlm = get_open_ai_lm(OpenAiModelNames.gpt_3_5_turbo_instruct)\nassert lm.estimate_tokens_in_prompt(\n    LmPrompt(\"My name is Spingldorph\", max_tokens=10)) == 7\nassert not lm.could_completion_go_over_token_limit(LmPrompt(\n    \"My name is Spingldorph\", max_tokens=1000))\n</code></pre>"},{"location":"#todos","title":"TODOs","text":"<p>If you are interested in one of these particular features or something else please make a Github Issue.</p> <ul> <li>[X] Openai completion</li> <li>[X] Openai chat</li> <li>[X] Huggingface interface</li> <li>[X] Huggingface device checking on PyTorch</li> <li>[X] Move cache to be per project</li> <li>[X] Redesign cache away from generic <code>diskcache</code> to make it easier to manage</li> <li>[X] Smart caching when num_completions &gt; 1 (reusing prior completions)</li> <li>[X] OpenAI batching interface (experimental)</li> <li>[ ] Anthropic interface</li> <li>[ ] Be able to add user metadata to a prompt</li> <li>[ ] Use the huggingface chat templates for chat models if available</li> <li>[ ] Automatic cache eviction to limit count or disk size (right now have to run a SQL query to delete entries before a certain time or matching your criteria)</li> <li>[ ] Multimodal/images in super easy format (like automatically process pil, opencv, etc)</li> <li>[ ] sort through usage of quantized models</li> <li>[ ] Cost estimation of a prompt before running / log total cost</li> <li>[ ] Additional Huggingface runtimes (TensorRT, BetterTransformers, etc)</li> <li>[ ] async / streaming (not a top priority for non-interactive research use cases)</li> <li>[ ] some lightweight utilities to help with tool use</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Wrappers for openai</p>"},{"location":"api/#lmwrapper.openai_wrapper.CompletionWindow","title":"<code>CompletionWindow</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The</p> Source code in <code>lmwrapper/batch_config.py</code> <pre><code>class CompletionWindow(StrEnum):\n    \"\"\"The\"\"\"\n\n    ASAP = \"asap\"\n    BATCH_ANY = \"batch_any\"\n    \"\"\"Uses the batch api willing to accept any latency.\n    What this means might depend on the API. For example,\n    OpenAI provides a 24hr target guarantee. However, if\n    the number of inputs exceeds the user's daily queue limit,\n    then this might have to be split over multiple days\n    (more usage limit can be purchased by adding credits).\n    Thus, this completion window doesn't make any guarantees,\n    but it is often moderately fast.\n    \"\"\"\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.CompletionWindow.BATCH_ANY","title":"<code>BATCH_ANY = 'batch_any'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Uses the batch api willing to accept any latency. What this means might depend on the API. For example, OpenAI provides a 24hr target guarantee. However, if the number of inputs exceeds the user's daily queue limit, then this might have to be split over multiple days (more usage limit can be purchased by adding credits). Thus, this completion window doesn't make any guarantees, but it is often moderately fast.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrediction","title":"<code>LmPrediction</code>  <code>dataclass</code>","text":"Source code in <code>lmwrapper/structs.py</code> <pre><code>@dataclass\nclass LmPrediction:\n    completion_text: str | None\n    \"\"\"The new text generated. It might be None if errors\"\"\"\n    prompt: LmPrompt\n    metad: Any\n    internals: ModelInternalsResults | None = field(default=None, kw_only=True)\n    error_message: str | None = field(default=None, kw_only=True)\n\n    def __post_init__(self):\n        if self.error_message is not None:\n            if not isinstance(self.error_message, str):\n                msg = \"The error_message parameter should be a string.\"\n                raise ValueError(msg)\n\n    @property\n    def has_errors(self):\n        return self.error_message is not None\n\n    @classmethod\n    def parse_from_cache(\n        cls,\n        completion_text: str,\n        prompt: LmPrompt,\n        metad_bytes: bytes,\n        error_message: str | None,\n    ):\n        return cls(\n            completion_text=completion_text,\n            prompt=prompt,\n            metad=pickle.loads(metad_bytes),\n            error_message=error_message,\n        )\n\n    def serialize_metad_for_cache(self) -&gt; bytes:\n        return pickle.dumps(self.metad)\n\n    def __post_init__(self):\n        self._was_cached = False\n\n    @property\n    def was_cached(self) -&gt; bool:\n        return hasattr(self, \"_was_cached\") and self._was_cached\n\n    def mark_as_cached(self) -&gt; \"LmPrediction\":\n        self._was_cached = True\n        return self\n\n    def _verify_logprobs(self):\n        if self.prompt.logprobs is None or self.prompt.logprobs == 0:\n            msg = \"This property is not available unless the prompt logprobs is set\"\n            raise ValueError(\n                msg,\n            )\n\n    @property\n    def completion_tokens(self) -&gt; list[str]:\n        msg = \"This version of prediction does not support completion tokens\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    @property\n    def completion_token_offsets(self):\n        msg = \"This version of prediction does not support completion token offsets\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    @property\n    def completion_logprobs(self) -&gt; list[float]:\n        msg = \"This version of prediction does not support completion logprobs\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    @property\n    def prompt_tokens(self):\n        msg = \"This version of prediction does not support prompt tokens\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    @property\n    def prompt_token_offsets(self):\n        msg = \"This version of prediction does not support prompt token offsets\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    @property\n    def prompt_logprobs(self):\n        msg = \"This version of prediction does not support prompt logprobs\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    def get_full_text(self):\n        return self.prompt.text + self.completion_text\n\n    def get_full_tokens(self):\n        msg = \"This version of prediction does not support full tokens\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    @property\n    def full_logprobs(self):\n        msg = \"This version of prediction does not support full logprobs\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    def completion_mean_logprob(self):\n        return statistics.mean(self.completion_logprobs)\n\n    @property\n    def top_token_logprobs(self) -&gt; list[dict[str, float]]:\n        if self.prompt.logprobs == 1:\n            # If we only have a single logprob, then we can adapt the\n            #   logprob property into a list of dictionaries. This allows\n            #   partial support for implementations that support only that.\n            return [\n                {token: float(logprob)}\n                for token, logprob in zip(\n                    self.completion_tokens,\n                    self.completion_logprobs,\n                    strict=True,\n                )\n            ]\n        msg = \"This version of prediction does not support top token logprobs\"\n        raise NotImplementedError(\n            msg,\n        )\n\n    def dict_serialize(\n        self,\n        pull_out_props: bool = True,\n        include_metad: bool = False,\n    ) -&gt; dict[str, Any]:\n        out = {\n            \"completion_text\": self.completion_text,\n            \"prompt\": self.prompt.dict_serialize(),\n            \"was_cached\": self.was_cached,\n            \"error_message\": self.error_message,\n        }\n        if pull_out_props:\n            with contextlib.suppress(Exception):\n                out[\"prompt_tokens\"] = self.prompt_tokens\n\n            with contextlib.suppress(Exception):\n                out[\"completion_tokens\"] = self.completion_tokens\n\n            with contextlib.suppress(Exception):\n                out[\"prompt_logprobs\"] = self.prompt_logprobs\n\n            with contextlib.suppress(Exception):\n                out[\"completion_logprobs\"] = self.completion_logprobs\n\n            with contextlib.suppress(Exception):\n                out[\"full_logprobs\"] = self.full_logprobs\n\n            with contextlib.suppress(Exception):\n                out[\"top_token_logprobs\"] = self.top_token_logprobs\n\n        if include_metad:\n            out[\"metad\"] = self.metad\n        return out\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrediction.completion_text","title":"<code>completion_text: str | None</code>  <code>instance-attribute</code>","text":"<p>The new text generated. It might be None if errors</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPredictor","title":"<code>LmPredictor</code>","text":"Source code in <code>lmwrapper/abstract_predictor.py</code> <pre><code>class LmPredictor:\n    _rate_limit: RateLimit | None = None\n\n    def __init__(\n        self,\n        cache_default: bool = False,\n    ):\n        self._cache_default = cache_default\n        # self._disk_cache = get_disk_cache()\n        from lmwrapper.sqlcache import SqlBackedCache\n\n        self._disk_cache = SqlBackedCache(self)\n\n    def find_prediction_class(self, prompt):\n        return LmPrediction\n\n    def predict(\n        self,\n        prompt: LmPrompt | str | LM_CHAT_DIALOG_COERCIBLE_TYPES,\n    ) -&gt; LmPrediction | list[LmPrediction]:\n        prompt = self._cast_prompt(prompt)\n        should_cache = self._cache_default if prompt.cache is None else prompt.cache\n        if should_cache and prompt.model_internals_request is not None:\n            raise NotImplementedError(\n                \"Cannot yet cache predictions with model internals request\",\n            )\n        self._validate_prompt(prompt, raise_on_invalid=True)\n        num_completions = prompt.num_completions or 1\n        if should_cache:\n            cached_vals = self._read_cached_values(prompt)\n            if len(cached_vals) &gt;= num_completions:\n                assert len(cached_vals) == num_completions\n                if prompt.num_completions is None:\n                    assert len(cached_vals) == 1\n                    return cached_vals[0]\n                return cached_vals\n            # There are some missing values. Let's predict for the missing ones.\n            need_new_completions = num_completions - len(cached_vals)\n            if need_new_completions != num_completions:\n                new_prompt = dataclasses.replace(\n                    prompt,\n                    num_completions=num_completions - len(cached_vals),\n                )\n            else:\n                new_prompt = prompt\n            new_vals = self._predict_maybe_cached(new_prompt)\n            # Add the new values we got\n            if not isinstance(new_vals, list):\n                new_vals = [new_vals]\n            for val in new_vals:\n                try:\n                    # self._disk_cache.set(cache_key, val)\n                    # TODO maybe figure out a way to bulk add\n                    self._disk_cache.add_or_set(val)\n                except OperationalError as e:\n                    print(\"Failed to cache\", e)\n            vals = cached_vals + new_vals\n        else:\n            vals = self._predict_maybe_cached(prompt)\n        if prompt.num_completions is None and isinstance(vals, list):\n            assert len(vals) &gt;= 1\n            return vals[0]\n        return vals\n\n    def _read_cached_values(self, prompt: LmPrompt) -&gt; list[LmPrediction]:\n        \"\"\"\n        Checks the cache for any matches of the prompt. Returns a list\n        as if num_completions is &gt;1 we might have multiple items\n        \"\"\"\n        cache_key = prompt\n        try:\n            cached_items = self._disk_cache.get(cache_key)\n        except OperationalError as e:\n            print(\"Failed to get from cache\", e)\n            cached_items = None\n        if not cached_items:\n            return []\n        for i, item in enumerate(cached_items):\n            if isinstance(item, BatchPredictionPlaceholder):\n                raise NotImplementedError(\n                    \"We retrieved a non-finalized batched prediction from\"\n                    \" the cache. This might be actually finished and we could\"\n                    \" recover and check to see if it is done. However, this is\"\n                    \" not yet implemented. For now, perhaps try to give this\"\n                    \" prompt to predict_many to retrieve the batch data.\",\n                )\n            cached_items[i] = item.mark_as_cached()\n        return cached_items\n\n    def predict_many(\n        self,\n        prompts: list[LmPrompt],\n        completion_window: CompletionWindow,\n    ) -&gt; Iterable[LmPrediction | list[LmPrediction]]:\n        self._validate_predict_many_prompts(prompts)\n        for prompt in prompts:\n            val = self.predict(prompt)\n            yield val\n\n    def _validate_predict_many_prompts(self, prompts):\n        if not isinstance(prompts, list):\n            msg = (\n                \"prompts input to predict_many must be a list of LmPrompt objects. \"\n                \"Got type: {type(prompts)}\"\n            )\n            raise ValueError(msg)\n        for i, prompt in enumerate(prompts):\n            if not isinstance(prompt, LmPrompt):\n                msg = (\n                    f\"prompts[{i}] must be a LmPrompt object. Got type: {type(prompt)}\"\n                )\n                raise ValueError(msg)\n\n    def remove_prompt_from_cache(\n        self,\n        prompt: str | LmPrompt,\n    ) -&gt; bool:\n        return self._disk_cache.delete(prompt)\n\n    def _validate_prompt(self, prompt: LmPrompt, raise_on_invalid: bool = True) -&gt; bool:\n        \"\"\"Called on prediction to make sure the prompt is valid for the model\"\"\"\n        return True\n\n    @abstractmethod\n    def get_model_cache_key(self):\n        return type(self).__name__\n\n    @abstractmethod\n    def _predict_maybe_cached(\n        self,\n        prompt: LmPrompt,\n    ) -&gt; list[LmPrediction]:\n        pass\n\n    def _cast_prompt(self, prompt: str | LmPrompt) -&gt; LmPrompt:\n        if isinstance(prompt, str):\n            return LmPrompt(prompt, 100)\n        if isinstance(prompt, list):\n            if any(isinstance(e, LmPrompt) for e in prompt):\n                msg = (\n                    \"The passed in prompt is a list that contains another prompt. This\"\n                    \" is not allowed. If you would like to predict multiple prompts,\"\n                    \" use the `predict_many` method.\"\n                )\n                raise ValueError(msg)\n            if self.is_chat_model:\n                return LmPrompt(prompt)\n            else:\n                msg = (\n                    \"Passing a list into `predict` is interpreted as a conversation\"\n                    f\" with multiple turns. However, this LM ({self.model_name()}) is\"\n                    \" not a chat model.\\n\\nIf you were instead intending to predict on\"\n                    \" multiple prompts, use the `predict_many` method.\"\n                )\n                raise ValueError(msg)\n        elif isinstance(prompt, LmPrompt):\n            return prompt\n        else:\n            msg = (\n                \"The prompt input should be a `LmPrompt`, a string, or if a chat\"\n                \" model, something coercible to a chat dialog. Got type:\"\n                f\" {type(prompt)}\"\n            )\n            raise ValueError(msg)\n\n    def estimate_tokens_in_prompt(self, prompt: LmPrompt) -&gt; int:\n        raise NotImplementedError\n\n    @property\n    def token_limit(self):\n        raise NotImplementedError\n\n    def could_completion_go_over_token_limit(self, prompt: LmPrompt) -&gt; bool:\n        count = self.estimate_tokens_in_prompt(prompt)\n        return (\n            count + (prompt.max_tokens or self.default_tokens_generated)\n        ) &gt; self.token_limit\n\n    def model_name(self):\n        return self.__class__.__name__\n\n    def remove_special_chars_from_tokens(self, tokens: list[str]) -&gt; list[str]:\n        \"\"\"\n        Certain tokenizers have special characters (such as a \u0120 to represent a space).\n        This method is to try to remove those and get it in a form that could be joined\n        and represent the original text.\n        \"\"\"\n        raise NotImplementedError\n\n    def tokenize(self, input_str: str) -&gt; list[str]:\n        msg = \"This predictor does not implement tokenization\"\n        raise NotImplementedError(msg)\n\n    def configure_global_ratelimit(\n        self,\n        max_count=1,\n        per_seconds=1,\n        greedy=False,\n    ) -&gt; None:\n        \"\"\"\n        Configure global ratelimiting, max tries per given seconds\n        If greedy is set to true, requests will be made without time inbetween,\n        followed by a long wait. Otherwise, requests are evenly spaced.\n        \"\"\"\n        if max_count and per_seconds:\n            LmPredictor._rate_limit = RateLimit(\n                max_count=max_count,\n                per=per_seconds,\n                greedy=greedy,\n            )\n        else:\n            LmPredictor._rate_limit = None\n\n        return LmPredictor._rate_limit\n\n    @classmethod\n    def _wait_ratelimit(cls) -&gt; float:\n        if LmPredictor._rate_limit:\n            return LmPredictor._rate_limit.wait()\n\n        return 0.0\n\n    @property\n    @abstractmethod\n    def is_chat_model(self) -&gt; bool:\n        raise NotImplementedError\n\n    @property\n    def default_tokens_generated(self) -&gt; int | None:\n        return self.token_limit // 16\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.LmPredictor.configure_global_ratelimit","title":"<code>configure_global_ratelimit(max_count=1, per_seconds=1, greedy=False)</code>","text":"<p>Configure global ratelimiting, max tries per given seconds If greedy is set to true, requests will be made without time inbetween, followed by a long wait. Otherwise, requests are evenly spaced.</p> Source code in <code>lmwrapper/abstract_predictor.py</code> <pre><code>def configure_global_ratelimit(\n    self,\n    max_count=1,\n    per_seconds=1,\n    greedy=False,\n) -&gt; None:\n    \"\"\"\n    Configure global ratelimiting, max tries per given seconds\n    If greedy is set to true, requests will be made without time inbetween,\n    followed by a long wait. Otherwise, requests are evenly spaced.\n    \"\"\"\n    if max_count and per_seconds:\n        LmPredictor._rate_limit = RateLimit(\n            max_count=max_count,\n            per=per_seconds,\n            greedy=greedy,\n        )\n    else:\n        LmPredictor._rate_limit = None\n\n    return LmPredictor._rate_limit\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.LmPredictor.remove_special_chars_from_tokens","title":"<code>remove_special_chars_from_tokens(tokens)</code>","text":"<p>Certain tokenizers have special characters (such as a \u0120 to represent a space). This method is to try to remove those and get it in a form that could be joined and represent the original text.</p> Source code in <code>lmwrapper/abstract_predictor.py</code> <pre><code>def remove_special_chars_from_tokens(self, tokens: list[str]) -&gt; list[str]:\n    \"\"\"\n    Certain tokenizers have special characters (such as a \u0120 to represent a space).\n    This method is to try to remove those and get it in a form that could be joined\n    and represent the original text.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt","title":"<code>LmPrompt</code>  <code>dataclass</code>","text":"Source code in <code>lmwrapper/structs.py</code> <pre><code>@dataclass(frozen=True)\nclass LmPrompt:\n    text: str | LM_CHAT_DIALOG_COERCIBLE_TYPES\n    \"\"\"The actual text of the prompt. If it is a LM_CHAT_DIALOG_COERCIBLE_TYPES\n    which can become a LmChatDialog (such as a list of strings) it will be converted\n    into a LmChatDialog.\"\"\"\n    max_tokens: int | None = None\n    \"\"\"The maximum number of tokens to generate in the completion. If `None`\n    then the downstream model will choose some default value. This value\n    might be a function of the prompt input length, but this behaviour is not defined.\n    This means it is possible that the default max might cause errors with long prompts.\n    It recommended that you specify a limit yourself to have more predictable\n    behaviour.\"\"\"\n    max_completion_tokens: int | None = None\n    \"\"\"Because some applications might rely on max_tokens matching the number of tokens received from the API, the o1 series introduces max_completion_tokens to explicitly control the total number of tokens generated by the model, including both reasoning and visible completion tokens. This explicit opt-in ensures no existing applications break when using the new models. The max_tokens parameter continues to function as before for all previous models.\"\"\"\n    stop: list[str] = None\n    \"\"\"Sequences where the model will stop generating further tokens.\n    The returned text will not contain the stop sequence. This sequence might span\n    accross tokens and does not have to be an actual token in the vocabulary.\n    For example could make a stop token of 'I like pie' even if that's not actually\n    a token.\n    \"\"\"\n    stop_mode: StopMode = StopMode.AUTO\n    \"\"\"Different models/providers handle stopping in different ways. You can\n    either leave that as-is (\"auto\") or try to change the mode to try\n    to emulate another mode (which may or may not work depending on\n    the model. Models are expected to raise an error if an incompatible\n    mode is given).\"\"\"\n    logprobs: int = 1\n    \"\"\"Include the log probabilities on the logprobs most likely tokens,\n    as well the chosen tokens. For example, if logprobs is 5, the\n    API will return a list of the 5 most likely tokens.\n    The model will always return the logprob of the sampled token,\n    so there may be up to logprobs+1 elements in the response.\n\n    In the case of openai the maximum value for logprobs is 5.\n    \"\"\"\n    temperature: float = 1.0\n    \"\"\"What sampling temperature to use, between 0 and 2.\n    Higher values like 0.8 will make the output more random, while lower values\n    like 0.2 will make it more focused and deterministic.\"\"\"\n    top_p: float = 1.0\n    \"\"\"An alternative to sampling with temperature, called nucleus sampling, where the\n    model considers the results of the tokens with top_p probability mass. So 0.1 means\n    only the tokens comprising the top 10% probability mass are considered.\n    If set to float &lt; 1, only the smallest set of most probable tokens with\n    probabilities that add up to top_p or higher are kept for generation.\"\"\"\n    presence_penalty: float = 0.0\n    \"\"\"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether\n    they appear in the text so far, increasing the model's likelihood\n    to talk about new topics. This parameter is used to encourage the model to include a\n    diverse range of tokens in the generated text. It is a value that is subtracted from\n    the log-probability of a token each time it is generated. A higher presence_penalty\n    value will result in the model being more likely to generate tokens that have not\n    yet been included in the generated text.\"\"\"\n    frequency_penalty: float = 0.0\n    \"\"\"Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n    existing frequency in the text so far, decreasing the model's likelihood\n    to repeat the same line verbatim. This parameter is used to discourage the model from\n    repeating the same words or phrases too frequently within the generated text.\n    It is a value that is added to the log-probability of a token each time it occurs in\n    the generated text. A higher frequency_penalty value will result in the model being\n    more conservative in its use of repeated tokens.\"\"\"\n    num_completions: int | None = None\n    \"\"\"How many completions to generate for each prompt. The default `None` will\n    just return a single LmPrediction object. If it is an integer than a list\n    of completion objects will be returned.\"\"\"\n    cache: bool = None\n    \"\"\"Whether to attempt to cache the model output. This overrides any default\n    settings of the model. This can be useful in saving computation but means\n    sampling might not work as expected. When it set to `None` it will use the\n    default of the predictor.\"\"\"\n    echo: bool = False\n    \"\"\"Whether to echo back the original prompt. Also allows you to get the\n    probability of the prompt under the model\"\"\"\n    add_bos_token: bool = None\n    \"\"\"Whether to add a bos (beginning-of-sentence) token at the beginning of the prompt.\n    Properly handling BOS tokens is important for several reasons\n       1. Some models might be trained this way. Their learned algorithms might depend\n          on the existence of the token, and we want to match the training setting.\n       2. Having a BOS allows for unconditional generation (ie, no prompt)\n       3. Having a BOS lets us have a probability even for the first token.\n    There are three states this could be (None, True, False).\n    By default (None), we will add a bos token if the tokenizer does not seem to\n    add one by default, and it is not a seq2seq model.\n    Set to True to always add a bos token.\n    Set to False to never add a bos token (but the tokenizer might still add one).\n    \"\"\"\n    add_special_tokens: bool = True\n    \"\"\"Whether or not to add special tokens when encoding the prompt.\"\"\"\n    model_internals_request: Optional[\"ModelInternalsRequest\"] = None\n    \"\"\"Used to attempt to get hidden states and attentions from the model.\"\"\"\n\n    # TODO: make a auto_reduce_max_tokens to reduce when might go over.\n\n    def __post_init__(self):\n        if isinstance(self.text, list):\n            # Convert the text into a chat dialog\n            object.__setattr__(self, \"text\", LmChatDialog(self.text))\n        if self.max_tokens is not None and not isinstance(self.max_tokens, int):\n            msg = \"The max_tokens parameter should be an int.\"\n            raise ValueError(msg)\n        if self.num_completions is not None and (\n            not isinstance(self.num_completions, int) or self.num_completions &lt;= 0\n        ):\n            msg = (\n                \"The num_completions parameter should be an \"\n                \"int greater than 0 or None (in which case a single \"\n                \"non-list item will be returned)\"\n            )\n            raise ValueError(msg)\n        if self.stop is not None:\n            if not isinstance(self.stop, list):\n                msg = \"The stop parameter should be a list of strings on where to stop.\"\n                raise ValueError(\n                    msg,\n                )\n            if not all(isinstance(x, str) for x in self.stop):\n                msg = \"The stop parameter should be a list of strings on where to stop.\"\n                raise ValueError(\n                    msg,\n                )\n        if isinstance(self.temperature, int):\n            object.__setattr__(self, \"temperature\", float(self.temperature))\n        if not isinstance(self.temperature, float):\n            msg = \"The temperature parameter should be a float.\"\n            raise ValueError(msg)\n        if self.temperature &lt; 0.0:\n            msg = \"The temperature parameter should be a positive float.\"\n            raise ValueError(msg)\n        if not isinstance(self.top_p, float):\n            msg = \"The top_p parameter should be a float.\"\n            raise ValueError(msg)\n        if not isinstance(self.presence_penalty, float):\n            msg = \"The presence_penalty parameter should be a float.\"\n            raise ValueError(msg)\n        if self.cache is not None and not isinstance(self.cache, bool):\n            msg = \"The cache parameter should be a bool.\"\n            raise ValueError(msg)\n        if self.logprobs is not None and not isinstance(self.logprobs, int):\n            msg = (\n                \"The logprob parameter should be int denoting number of probs return,\"\n                \" or None.\"\n            )\n            raise ValueError(\n                msg,\n            )\n        if self.stop_mode != StopMode.AUTO:\n            raise NotImplementedError(\n                \"Only StopMode.AUTO is supported at this time as a temporary hack\",\n            )\n\n    def is_deterministic_sampling(self) -&gt; bool:\n        return (self.temperature &lt; 1e-4) or (self.top_p &lt; 1e-4)\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new prompt with the given parameters replaced.\"\"\"\n        return dataclasses.replace(self, **kwargs)\n\n    def is_text_a_chat(self) -&gt; bool:\n        return isinstance(self.text, LmChatDialog)\n\n    def get_text_as_chat(self) -&gt; \"LmChatDialog\":\n        return LmChatDialog(self.text)\n\n    def get_text_as_string_default_form(self) -&gt; str:\n        \"\"\"\n        Will always return a string, even if it was originally a chat. It will use\n        the default form of the chat specified in LmChatDialog.to_default_string_prompt()\n        \"\"\"\n        if self.is_text_a_chat():\n            return self.text.to_default_string_prompt()\n        else:\n            return self.text\n\n    def dict_serialize(self) -&gt; dict:\n        \"\"\"\n        Serialize the prompt into a json-compatible dictionary. Note this is not\n        guaranteed to be the same as the JSON representation for use\n        in an openai api call. This is just for serialization purposes.\n        \"\"\"\n        out = {\n            \"max_tokens\": self.max_tokens,\n            \"stop\": self.stop,\n            \"logprobs\": self.logprobs,\n            \"temperature\": self.temperature,\n            \"top_p\": self.top_p,\n            \"presence_penalty\": self.presence_penalty,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"num_completions\": self.num_completions,\n            \"cache\": self.cache,\n            \"echo\": self.echo,\n            \"add_bos_token\": self.add_bos_token,\n            \"add_special_tokens\": self.add_special_tokens,\n        }\n        if self.is_text_a_chat():\n            out[\"text\"] = self.get_text_as_chat().as_dicts()\n        else:\n            out[\"text\"] = self.text\n        return out\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.add_bos_token","title":"<code>add_bos_token: bool = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to add a bos (beginning-of-sentence) token at the beginning of the prompt. Properly handling BOS tokens is important for several reasons    1. Some models might be trained this way. Their learned algorithms might depend       on the existence of the token, and we want to match the training setting.    2. Having a BOS allows for unconditional generation (ie, no prompt)    3. Having a BOS lets us have a probability even for the first token. There are three states this could be (None, True, False). By default (None), we will add a bos token if the tokenizer does not seem to add one by default, and it is not a seq2seq model. Set to True to always add a bos token. Set to False to never add a bos token (but the tokenizer might still add one).</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.add_special_tokens","title":"<code>add_special_tokens: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether or not to add special tokens when encoding the prompt.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.cache","title":"<code>cache: bool = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to attempt to cache the model output. This overrides any default settings of the model. This can be useful in saving computation but means sampling might not work as expected. When it set to <code>None</code> it will use the default of the predictor.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.echo","title":"<code>echo: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to echo back the original prompt. Also allows you to get the probability of the prompt under the model</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.frequency_penalty","title":"<code>frequency_penalty: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. This parameter is used to discourage the model from repeating the same words or phrases too frequently within the generated text. It is a value that is added to the log-probability of a token each time it occurs in the generated text. A higher frequency_penalty value will result in the model being more conservative in its use of repeated tokens.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.logprobs","title":"<code>logprobs: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The model will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.</p> <p>In the case of openai the maximum value for logprobs is 5.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.max_completion_tokens","title":"<code>max_completion_tokens: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Because some applications might rely on max_tokens matching the number of tokens received from the API, the o1 series introduces max_completion_tokens to explicitly control the total number of tokens generated by the model, including both reasoning and visible completion tokens. This explicit opt-in ensures no existing applications break when using the new models. The max_tokens parameter continues to function as before for all previous models.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.max_tokens","title":"<code>max_tokens: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The maximum number of tokens to generate in the completion. If <code>None</code> then the downstream model will choose some default value. This value might be a function of the prompt input length, but this behaviour is not defined. This means it is possible that the default max might cause errors with long prompts. It recommended that you specify a limit yourself to have more predictable behaviour.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.model_internals_request","title":"<code>model_internals_request: Optional[ModelInternalsRequest] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Used to attempt to get hidden states and attentions from the model.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.num_completions","title":"<code>num_completions: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many completions to generate for each prompt. The default <code>None</code> will just return a single LmPrediction object. If it is an integer than a list of completion objects will be returned.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.presence_penalty","title":"<code>presence_penalty: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. This parameter is used to encourage the model to include a diverse range of tokens in the generated text. It is a value that is subtracted from the log-probability of a token each time it is generated. A higher presence_penalty value will result in the model being more likely to generate tokens that have not yet been included in the generated text.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.stop","title":"<code>stop: list[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sequences where the model will stop generating further tokens. The returned text will not contain the stop sequence. This sequence might span accross tokens and does not have to be an actual token in the vocabulary. For example could make a stop token of 'I like pie' even if that's not actually a token.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.stop_mode","title":"<code>stop_mode: StopMode = StopMode.AUTO</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Different models/providers handle stopping in different ways. You can either leave that as-is (\"auto\") or try to change the mode to try to emulate another mode (which may or may not work depending on the model. Models are expected to raise an error if an incompatible mode is given).</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.temperature","title":"<code>temperature: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.text","title":"<code>text: str | LM_CHAT_DIALOG_COERCIBLE_TYPES</code>  <code>instance-attribute</code>","text":"<p>The actual text of the prompt. If it is a LM_CHAT_DIALOG_COERCIBLE_TYPES which can become a LmChatDialog (such as a list of strings) it will be converted into a LmChatDialog.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.top_p","title":"<code>top_p: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. If set to float &lt; 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.</p>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.dict_serialize","title":"<code>dict_serialize()</code>","text":"<p>Serialize the prompt into a json-compatible dictionary. Note this is not guaranteed to be the same as the JSON representation for use in an openai api call. This is just for serialization purposes.</p> Source code in <code>lmwrapper/structs.py</code> <pre><code>def dict_serialize(self) -&gt; dict:\n    \"\"\"\n    Serialize the prompt into a json-compatible dictionary. Note this is not\n    guaranteed to be the same as the JSON representation for use\n    in an openai api call. This is just for serialization purposes.\n    \"\"\"\n    out = {\n        \"max_tokens\": self.max_tokens,\n        \"stop\": self.stop,\n        \"logprobs\": self.logprobs,\n        \"temperature\": self.temperature,\n        \"top_p\": self.top_p,\n        \"presence_penalty\": self.presence_penalty,\n        \"frequency_penalty\": self.frequency_penalty,\n        \"num_completions\": self.num_completions,\n        \"cache\": self.cache,\n        \"echo\": self.echo,\n        \"add_bos_token\": self.add_bos_token,\n        \"add_special_tokens\": self.add_special_tokens,\n    }\n    if self.is_text_a_chat():\n        out[\"text\"] = self.get_text_as_chat().as_dicts()\n    else:\n        out[\"text\"] = self.text\n    return out\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.get_text_as_string_default_form","title":"<code>get_text_as_string_default_form()</code>","text":"<p>Will always return a string, even if it was originally a chat. It will use the default form of the chat specified in LmChatDialog.to_default_string_prompt()</p> Source code in <code>lmwrapper/structs.py</code> <pre><code>def get_text_as_string_default_form(self) -&gt; str:\n    \"\"\"\n    Will always return a string, even if it was originally a chat. It will use\n    the default form of the chat specified in LmChatDialog.to_default_string_prompt()\n    \"\"\"\n    if self.is_text_a_chat():\n        return self.text.to_default_string_prompt()\n    else:\n        return self.text\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.LmPrompt.replace","title":"<code>replace(**kwargs)</code>","text":"<p>Returns a new prompt with the given parameters replaced.</p> Source code in <code>lmwrapper/structs.py</code> <pre><code>def replace(self, **kwargs):\n    \"\"\"Returns a new prompt with the given parameters replaced.\"\"\"\n    return dataclasses.replace(self, **kwargs)\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAIPredictor","title":"<code>OpenAIPredictor</code>","text":"<p>               Bases: <code>LmPredictor</code></p> Source code in <code>lmwrapper/openai_wrapper/wrapper.py</code> <pre><code>class OpenAIPredictor(LmPredictor):\n    _instantiation_hooks: list[\"OpenAiInstantiationHook\"] = []\n\n    def __init__(\n        self,\n        api: OpenAI,\n        engine_name: str,\n        chat_mode: bool | None = None,\n        o1_mode: bool | None = None,\n        cache_outputs_default: bool = False,\n        retry_on_rate_limit: bool = False,\n    ):\n        for hook in self._instantiation_hooks:\n            hook.before_init(\n                self,\n                api,\n                engine_name,\n                chat_mode,\n                cache_outputs_default,\n                retry_on_rate_limit,\n            )\n        super().__init__(cache_outputs_default)\n        self._api = api\n        self._engine_name = engine_name\n        self._cache_outputs_default = cache_outputs_default\n        self._retry_on_rate_limit = retry_on_rate_limit\n        info = OpenAiModelNames.name_to_info(engine_name)\n        self._chat_mode = info.is_chat_model if chat_mode is None else chat_mode\n        self._o1_mode = info._is_o1_model if o1_mode is None else o1_mode\n        if self._chat_mode is None:\n            msg = (\n                \"`chat_mode` is not provided as a parameter and cannot be inferred from\"\n                \" engine name\"\n            )\n            raise ValueError(\n                msg,\n            )\n        self._token_limit = info.token_limit if info is not None else None\n        self._tokenizer = None\n\n    @classmethod\n    def add_instantiation_hook(cls, hook: \"OpenAiInstantiationHook\"):\n        \"\"\"\n        This method should likely not be used normally.\n        It is intended add constraints on kinds of models that are\n        instantiation to better control usage. An example usage checking\n        keys are used correctly like that certain keys are used with particular\n        models\n        \"\"\"\n        cls._instantiation_hooks.append(hook)\n\n    def find_prediction_class(self, prompt):\n        # if self._chat_mode:\n        #    return OpenAiLmChatPrediction\n        return OpenAiLmPrediction\n\n    def _validate_prompt(self, prompt: LmPrompt, raise_on_invalid: bool = True) -&gt; bool:\n        if prompt.logprobs is not None and prompt.logprobs &gt; MAX_LOG_PROB_PARM:\n            message = (\n                f\"Openai limits logprobs to be &lt;= {MAX_LOG_PROB_PARM}. Larger values\"\n                \" might cause unexpected behavior if you later are dependingon more\"\n                \" returns\"\n            )\n            if raise_on_invalid:\n                raise ValueError(message)\n            else:\n                warnings.warn(message)\n                return False\n        if self._o1_mode:\n            # if prompt.max_tokens:\n            #     message = f\"o1 type models use `max_completion_tokens` instead of `max_tokens` but you have set max tokens to f{prompt.max_tokens}. Instead, `max_completion_tokens` will be used. `max_completion_tokens` is currently set to: {prompt.max_completion_tokens}\"\n            #     if raise_on_invalid:\n            #         raise ValueError(message)\n            #     else:\n            #         warnings.warn(message)\n            #         return False\n            if prompt.logprobs is not None and prompt.logprobs &gt; 0:\n                message = f\"logprobs is set to {prompt.logprobs} but o1 models do not support logprobs. This will be ignored and a value of `None` will be used instead.\"\n                if raise_on_invalid:\n                    raise ValueError(message)\n                else:\n                    warnings.warn(message)\n            if prompt.temperature != 1.0:\n                message = f\"temperature is set to {prompt.temperature} but o1 models do not support temperature. This will be ignored and a value of 1.0 will be used instead.\"\n                if raise_on_invalid:\n                    raise ValueError(message)\n                else:\n                    warnings.warn(message)\n                    return False\n            if prompt.stop:\n                message = f\"stop is set to {prompt.stop} but o1 models do not support stop tokens. This will be ignored.\"\n                if raise_on_invalid:\n                    raise ValueError(message)\n                else:\n                    warnings.warn(message)\n                    return False\n        return True\n\n    def model_name(self):\n        return self._engine_name\n\n    def get_model_cache_key(self):\n        return \"OpenAI::\" + self._engine_name\n\n    def list_engines(self):\n        return self._api.Engine.list()\n\n    @property\n    def is_chat_model(self):\n        return self._chat_mode\n\n    @property\n    def is_o1_model(self):\n        return self._o1_mode\n\n    @property\n    def token_limit(self):\n        return self._token_limit\n\n    def _build_tokenizer(self):\n        if self._tokenizer is None:\n            self._tokenizer = tiktoken.encoding_for_model(self._engine_name)\n\n    def tokenize_ids(self, input_str: str) -&gt; list[int]:\n        self._build_tokenizer()\n        return self._tokenizer.encode(input_str)\n\n    def tokenize(self, input_str: str) -&gt; list[str]:\n        self._build_tokenizer()\n        token_bytes: list[bytes] = self._tokenizer.decode_tokens_bytes(\n            self.tokenize_ids(input_str),\n        )\n        return [tok.decode(\"utf-8\") for tok in token_bytes]\n\n    def estimate_tokens_in_prompt(self, prompt: LmPrompt) -&gt; int:\n        \"\"\"\n        Estimate the number of tokens in the prompt.\n        This is not always an exact measure, as for the chat models there extra metadata provided.\n        The documentation on ChatMl (https://github.com/openai/openai-python/blob/main/chatml.md)\n        gives some details but is imprecise. We want to write this to ideally overestimate the\n        number of tokens so that will conservatively not go over the limit.\n        \"\"\"\n        self._build_tokenizer()\n        if self._chat_mode:\n            val = len(\n                self._tokenizer.encode(\n                    prompt.get_text_as_chat().to_default_string_prompt(),\n                ),\n            )\n            val += (\n                len(prompt.get_text_as_chat()) * 3\n            )  # Extra buffer for each turn transition\n            val += 2  # Extra setup tokens\n        else:\n            val = len(self._tokenizer.encode(prompt.get_text_as_string_default_form()))\n        return val\n\n    def _predict_maybe_cached(\n        self,\n        prompt: LmPrompt,\n    ) -&gt; LmPrediction | list[LmPrediction]:\n        if PRINT_ON_PREDICT:\n            print(\"RUN PREDICT \", prompt.text[: min(10, len(prompt.text))])\n\n        def run_func():\n            # Wait for rate limit\n            LmPredictor._wait_ratelimit()\n            args = prompt_to_openai_args_dict(\n                prompt,\n                self._engine_name,\n                self._chat_mode,\n                self._o1_mode,\n                default_tokens_generated=self.default_tokens_generated,\n            )\n\n            try:\n                if not self._chat_mode:\n                    return self._api.completions.create(**args)\n                else:\n                    return self._api.chat.completions.create(**args)\n            except RateLimitError as e:\n                print(e)\n                return e\n\n        def is_success_func(result):\n            return not isinstance(result, RateLimitError)\n\n        if self._retry_on_rate_limit:\n            completion = attempt_with_exponential_backoff(\n                run_func,\n                is_success_func,\n                backoff_time=parse_backoff_time,\n            )\n        else:\n            completion = run_func()\n\n        if not is_success_func(completion):\n            raise completion\n\n        preds = self.prediction_from_api_response(completion, prompt)\n        # if prompt.num_completions is None:\n        #    return preds[0]\n        return preds\n\n    def prediction_from_api_response(\n        self,\n        response: openai.types.Completion | openai.types.chat.ChatCompletion,\n        prompt: LmPrompt,\n    ) -&gt; list[LmPrediction]:\n        choices = response.choices\n\n        def get_completion_text(text):\n            if not prompt.echo:\n                return text\n            return text[len(prompt.text) :]\n\n        def get_text_from_choice(choice):\n            return choice.text if not self._chat_mode else choice.message.content\n\n        return [\n            OpenAiLmPrediction(\n                get_completion_text(get_text_from_choice(choice)),\n                prompt,\n                choice,\n                internals=None,\n                # tokenizer=self._tokenizer\n            )\n            for choice in choices\n        ]\n\n    def predict_many(\n        self,\n        prompts: list[LmPrompt],\n        completion_window: CompletionWindow,\n    ) -&gt; Iterable[LmPrediction | list[LmPrediction]]:\n        self._validate_predict_many_prompts(prompts)\n        if completion_window == CompletionWindow.BATCH_ANY:\n            from lmwrapper.openai_wrapper import batching\n\n            # ^ putting this here to prevent circular import.\n            #   probably some more clever way...\n            batch_manager = batching.OpenAiBatchManager(\n                prompts=prompts,\n                cache=self._disk_cache,\n                maintain_order=True,\n            )\n            batch_manager.start_batch()\n            yield from batch_manager.iter_results()\n            return\n        yield from super().predict_many(prompts, completion_window)\n\n    def remove_special_chars_from_tokens(self, tokens: list[str]) -&gt; list[str]:\n        return tokens\n\n    @property\n    def default_tokens_generated(self) -&gt; int | None:\n        # For the completion models it used to 16.\n        # https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens\n        # However, now there does not appear to be a limit for the chat models\n        # https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_completion_tokens\n        # We'll just switch over to no limit (or the total limit of the model)\n        if not self.is_chat_model:\n            return 16\n        return None\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAIPredictor.add_instantiation_hook","title":"<code>add_instantiation_hook(hook)</code>  <code>classmethod</code>","text":"<p>This method should likely not be used normally. It is intended add constraints on kinds of models that are instantiation to better control usage. An example usage checking keys are used correctly like that certain keys are used with particular models</p> Source code in <code>lmwrapper/openai_wrapper/wrapper.py</code> <pre><code>@classmethod\ndef add_instantiation_hook(cls, hook: \"OpenAiInstantiationHook\"):\n    \"\"\"\n    This method should likely not be used normally.\n    It is intended add constraints on kinds of models that are\n    instantiation to better control usage. An example usage checking\n    keys are used correctly like that certain keys are used with particular\n    models\n    \"\"\"\n    cls._instantiation_hooks.append(hook)\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAIPredictor.estimate_tokens_in_prompt","title":"<code>estimate_tokens_in_prompt(prompt)</code>","text":"<p>Estimate the number of tokens in the prompt. This is not always an exact measure, as for the chat models there extra metadata provided. The documentation on ChatMl (https://github.com/openai/openai-python/blob/main/chatml.md) gives some details but is imprecise. We want to write this to ideally overestimate the number of tokens so that will conservatively not go over the limit.</p> Source code in <code>lmwrapper/openai_wrapper/wrapper.py</code> <pre><code>def estimate_tokens_in_prompt(self, prompt: LmPrompt) -&gt; int:\n    \"\"\"\n    Estimate the number of tokens in the prompt.\n    This is not always an exact measure, as for the chat models there extra metadata provided.\n    The documentation on ChatMl (https://github.com/openai/openai-python/blob/main/chatml.md)\n    gives some details but is imprecise. We want to write this to ideally overestimate the\n    number of tokens so that will conservatively not go over the limit.\n    \"\"\"\n    self._build_tokenizer()\n    if self._chat_mode:\n        val = len(\n            self._tokenizer.encode(\n                prompt.get_text_as_chat().to_default_string_prompt(),\n            ),\n        )\n        val += (\n            len(prompt.get_text_as_chat()) * 3\n        )  # Extra buffer for each turn transition\n        val += 2  # Extra setup tokens\n    else:\n        val = len(self._tokenizer.encode(prompt.get_text_as_string_default_form()))\n    return val\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiInstantiationHook","title":"<code>OpenAiInstantiationHook</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Potentially used to add API controls on predictor instantiation. An example usecase is to make sure certain keys are only used with certain models.</p> Source code in <code>lmwrapper/openai_wrapper/wrapper.py</code> <pre><code>class OpenAiInstantiationHook(ABC):\n    \"\"\"\n    Potentially used to add API controls on predictor instantiation.\n    An example usecase is to make sure certain keys are only used with\n    certain models.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def before_init(\n        self,\n        new_predictor: OpenAIPredictor,\n        api,\n        engine_name: str,\n        chat_mode: bool,\n        cache_outputs_default: bool,\n        retry_on_rate_limit: bool,\n    ):\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiLmPrediction","title":"<code>OpenAiLmPrediction</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LmPrediction</code></p> Source code in <code>lmwrapper/openai_wrapper/wrapper.py</code> <pre><code>@dataclasses.dataclass\nclass OpenAiLmPrediction(LmPrediction):\n    # tokenizer: tiktoken.Encoding = dataclasses.field(default=None, kw_only=False)\n\n    def _get_completion_token_index(self):\n        \"\"\"\n        If echoing the completion text might not start at the begining. Returns the\n        index of the actually new tokens\n        \"\"\"\n        if not self.prompt.echo:\n            return 0\n        # binary search for the first token after the prompt length\n        prompt_len = len(self.prompt.text) - 1\n        all_offsets: list[int] = self._all_toks_offsets()\n        return bisect.bisect_right(all_offsets, prompt_len)\n\n    def _all_toks(self):\n        if not self.prompt.logprobs:\n            msg = (\n                \"This property is only available when the prompt `logprobs` flag is set\"\n                \" (openai endpoint only will return tokens when logprobs is set)\"\n            )\n            raise ValueError(\n                msg,\n            )\n        probably_chat = hasattr(self.metad.logprobs, \"content\")\n        if not probably_chat:\n            return self.metad.logprobs.tokens\n        else:\n            return [lp.token for lp in self.metad.logprobs.content]\n\n    def _all_toks_offsets(self):\n        return self.metad.logprobs.text_offset\n\n    def _all_logprobs(self):\n        if self.metad.logprobs is None:\n            assert self.prompt.logprobs is None or self.prompt.logprobs == 0\n            return None\n        probably_chat = hasattr(self.metad.logprobs, \"content\")\n        if not probably_chat:\n            return self.metad.logprobs.token_logprobs\n        else:\n            return [lp.logprob for lp in self.metad.logprobs.content]\n\n    @property\n    def completion_tokens(self):\n        return self._all_toks()[self._get_completion_token_index() :]\n\n    @property\n    def completion_token_offsets(self):\n        return self._all_toks_offsets()[self._get_completion_token_index() :]\n\n    @property\n    def completion_logprobs(self):\n        \"\"\"Note that this will only be valid if set a logprob value in the prompt\"\"\"\n        self._verify_logprobs()\n        return self._all_logprobs()[self._get_completion_token_index() :]\n\n    def _verify_echo(self):\n        if not self.prompt.echo:\n            msg = \"This property is only available when the prompt `echo` flag is set\"\n            raise ValueError(\n                msg,\n            )\n\n    @property\n    def prompt_tokens(self):\n        self._verify_echo()\n        return self._all_toks()[: self._get_completion_token_index()]\n\n    @property\n    def prompt_token_offsets(self):\n        self._verify_echo()\n        return self._all_toks_offsets()[: self._get_completion_token_index()]\n\n    @property\n    def prompt_logprobs(self):\n        self._verify_echo()\n        self._verify_logprobs()\n        return self._all_logprobs()[: self._get_completion_token_index()]\n\n    @property\n    def full_logprobs(self):\n        return self.prompt_logprobs + self.completion_logprobs\n\n    def get_full_tokens(self):\n        return self.prompt_tokens + self.completion_tokens\n\n    def get_full_text(self):\n        return self.prompt.text + self.completion_text\n\n    @property\n    def logprobs_dict(self):\n        return [\n            {\n                \"repr\": repr(token),\n                \"probability\": logprob,\n            }\n            for token, logprob in zip(\n                self.completion_tokens,\n                self.completion_logprobs,\n                strict=True,\n            )\n        ]\n\n    @property\n    def top_token_logprobs(self) -&gt; list[dict[str, float]]:\n        \"\"\"\n        List of dictionaries of token:logprob for each completion.\n        The API will always return the logprob of the sampled token,\n        so there may be up to logprobs+1 elements in the response.\n        \"\"\"\n        if self.metad.logprobs is None:\n            msg = (\n                \"Response does not contain top_logprobs. Are you sure logprobs was set\"\n                f\" &gt; 0? Currently: {self.prompt.logprobs}\"\n            )\n            raise ValueError(\n                msg,\n            )\n\n        if isinstance(self.metad.logprobs, Logprobs):\n            return self.metad.logprobs.top_logprobs\n\n        top_logprobs = []\n        for p in self.metad.logprobs.content:  # for each token\n            odict = dict([(t.token, t.logprob) for t in p.top_logprobs])\n            top_logprobs.append(odict)\n        return top_logprobs\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiLmPrediction.completion_logprobs","title":"<code>completion_logprobs</code>  <code>property</code>","text":"<p>Note that this will only be valid if set a logprob value in the prompt</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiLmPrediction.top_token_logprobs","title":"<code>top_token_logprobs: list[dict[str, float]]</code>  <code>property</code>","text":"<p>List of dictionaries of token:logprob for each completion. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames","title":"<code>OpenAiModelNames</code>","text":"<p>Enum for available OpenAI models. Variable docstrings adapted from documentation on OpenAI's website at the time.</p> Source code in <code>lmwrapper/openai_wrapper/wrapper.py</code> <pre><code>class OpenAiModelNames(metaclass=_ModelNamesMeta):\n    \"\"\"\n    Enum for available OpenAI models. Variable docstrings adapted from\n    documentation on OpenAI's website at the time.\n    \"\"\"\n\n    gpt_3_5_turbo = OpenAiModelInfo(\"gpt-3.5-turbo\", True, False, 4096)\n    \"\"\"Most capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003.\n    Will be updated with our latest model iteration 2 weeks after it is released.\"\"\"\n    gpt_3_5_turbo_16k = OpenAiModelInfo(\"gpt-3.5-turbo-16k\", True, False, 16384)\n    \"\"\"Same capabilities as the standard gpt-3.5-turbo model but with 4 times the context.\"\"\"\n    gpt_3_5_turbo_instruct = OpenAiModelInfo(\n        \"gpt-3.5-turbo-instruct\", False, False, 4096\n    )\n    \"\"\"A GPT-3.5 version but for completion\"\"\"\n    code_davinci_002 = OpenAiModelInfo(\"code-davinci-002\", False, False, 4097)\n    \"\"\"Can do any language task with better quality, longer output, and consistent instruction-following\n    than the curie, babbage, or ada models.\n    Also supports some additional features such as inserting text.\"\"\"\n    gpt_4 = OpenAiModelInfo(\"gpt-4\", True, False, 8192)\n    \"\"\"More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat.\n    Will be updated with our latest model iteration 2 weeks after it is released.\"\"\"\n    gpt_4_32k = OpenAiModelInfo(\"gpt-4-32k\", True, False, 32768)\n    \"\"\"Same capabilities as the base gpt-4 mode but with 4x the context length.\n    Will be updated with our latest model iteration.\"\"\"\n    gpt_4_turbo = OpenAiModelInfo(\"gpt-4-1106-preview\", True, False, 128_000)\n    \"\"\"GPT-4 model with improved instruction following, JSON mode,\n    reproducible outputs, parallel function calling, and more.\n    Returns a maximum of 4,096 output tokens. This preview model is\n    not yet suited for production traffic.\n\n    Note that we don't currently handle the differing input and output\n    token limits (tracked #25).\n\n    see: https://help.openai.com/en/articles/8555510-gpt-4-turbo\n    \"\"\"\n    gpt_4o = OpenAiModelInfo(\"gpt-4o\", True, False, 128_000)\n    \"\"\"\n    GPT-4o (\u201co\u201d for \u201comni\u201d) is our most advanced model. \n    It is multimodal (accepting text or image inputs and outputting text), \n    and it has the same high intelligence as GPT-4 Turbo but \n    is much more efficient\u2014it generates text 2x faster and is 50% cheaper. \n    Additionally, GPT-4o has the best vision and performance across \n    non-English languages of any of our models. GPT-4o is available in \n    the OpenAI API to paying customers.\n\n    Point can change\n    \"\"\"\n    gpt_4o_2024_05_13 = OpenAiModelInfo(\"gpt-4o-2024-05-13\", True, False, 128_000)\n    gpt_4o_mini = OpenAiModelInfo(\"gpt-4o-mini\", True, False, 128_000)\n    \"\"\"Our affordable and intelligent small model for fast, lightweight tasks. \n    GPT-4o mini is cheaper and more capable than GPT-3.5 Turbo. \"\"\"\n    gpt_4o_mini_2024_07_18 = OpenAiModelInfo(\n        \"gpt-4o-mini-2024-07-18\", True, False, 128_000\n    )\n\n    \"\"\"\n    The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.\n\n    Previews may change. Points to o1-preview-2024-09-12\tas of 2024-10-01.\n    \"\"\"\n    \"\"\"o1-preview: reasoning model designed to solve hard problems across domains.\"\"\"\n    o1_preview = OpenAiModelInfo(\"o1-preview\", True, True, 128_000)\n    o1_preview_2024_09_12 = OpenAiModelInfo(\n        \"o1-preview-2024-09-12\", True, True, 128_000\n    )\n    \"\"\"o1-mini: faster and cheaper reasoning model particularly good at coding, math, and science.\"\"\"\n    o1_mini = OpenAiModelInfo(\"o1-mini\", True, True, 128_000)\n    o1_mini_2024_09_12 = OpenAiModelInfo(\"o1-mini-2024-09-12\", True, True, 128_000)\n\n    @classmethod\n    def name_to_info(cls, name: str) -&gt; OpenAiModelInfo | None:\n        if isinstance(name, OpenAiModelInfo):\n            return name\n        for info in cls:\n            if info == name:\n                return info\n        return None\n</code></pre>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.code_davinci_002","title":"<code>code_davinci_002 = OpenAiModelInfo('code-davinci-002', False, False, 4097)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports some additional features such as inserting text.</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_3_5_turbo","title":"<code>gpt_3_5_turbo = OpenAiModelInfo('gpt-3.5-turbo', True, False, 4096)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Most capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration 2 weeks after it is released.</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_3_5_turbo_16k","title":"<code>gpt_3_5_turbo_16k = OpenAiModelInfo('gpt-3.5-turbo-16k', True, False, 16384)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Same capabilities as the standard gpt-3.5-turbo model but with 4 times the context.</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_3_5_turbo_instruct","title":"<code>gpt_3_5_turbo_instruct = OpenAiModelInfo('gpt-3.5-turbo-instruct', False, False, 4096)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A GPT-3.5 version but for completion</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_4","title":"<code>gpt_4 = OpenAiModelInfo('gpt-4', True, False, 8192)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration 2 weeks after it is released.</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_4_32k","title":"<code>gpt_4_32k = OpenAiModelInfo('gpt-4-32k', True, False, 32768)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Same capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_4_turbo","title":"<code>gpt_4_turbo = OpenAiModelInfo('gpt-4-1106-preview', True, False, 128000)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. This preview model is not yet suited for production traffic.</p> <p>Note that we don't currently handle the differing input and output token limits (tracked #25).</p> <p>see: https://help.openai.com/en/articles/8555510-gpt-4-turbo</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_4o","title":"<code>gpt_4o = OpenAiModelInfo('gpt-4o', True, False, 128000)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>GPT-4o (\u201co\u201d for \u201comni\u201d) is our most advanced model.  It is multimodal (accepting text or image inputs and outputting text),  and it has the same high intelligence as GPT-4 Turbo but  is much more efficient\u2014it generates text 2x faster and is 50% cheaper.  Additionally, GPT-4o has the best vision and performance across  non-English languages of any of our models. GPT-4o is available in  the OpenAI API to paying customers.</p> <p>Point can change</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_4o_mini","title":"<code>gpt_4o_mini = OpenAiModelInfo('gpt-4o-mini', True, False, 128000)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Our affordable and intelligent small model for fast, lightweight tasks.  GPT-4o mini is cheaper and more capable than GPT-3.5 Turbo.</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.gpt_4o_mini_2024_07_18","title":"<code>gpt_4o_mini_2024_07_18 = OpenAiModelInfo('gpt-4o-mini-2024-07-18', True, False, 128000)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</p> <p>Previews may change. Points to o1-preview-2024-09-12        as of 2024-10-01.</p>"},{"location":"api/#lmwrapper.openai_wrapper.OpenAiModelNames.o1_preview_2024_09_12","title":"<code>o1_preview_2024_09_12 = OpenAiModelInfo('o1-preview-2024-09-12', True, True, 128000)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>o1-mini: faster and cheaper reasoning model particularly good at coding, math, and science.</p>"},{"location":"api/#lmwrapper.openai_wrapper.attempt_with_exponential_backoff","title":"<code>attempt_with_exponential_backoff(call_func, is_success_func, backoff_time=None, backoff_cap=60)</code>","text":"<p>Attempts to get a result from call_func. Uses is_success_func to determine if the result was a success or not. If not a success then will sleep for a random amount of time between 1 and 2^attempts</p> Source code in <code>lmwrapper/openai_wrapper/wrapper.py</code> <pre><code>def attempt_with_exponential_backoff(\n    call_func,\n    is_success_func,\n    backoff_time=None,\n    backoff_cap=60,\n):\n    \"\"\"\n    Attempts to get a result from call_func. Uses is_success_func\n    to determine if the result was a success or not. If not a success\n    then will sleep for a random amount of time between 1 and 2^attempts\n    \"\"\"\n    result = call_func()\n    attempts = 1\n    sleep_time = False\n    while not is_success_func(result):\n        if backoff_time:\n            sleep_time = backoff_time(result)\n        if not backoff_time or not sleep_time:\n            sleep_time = random.randint(1, min(2**attempts, backoff_cap))\n        print(f\"Rate limit error. Sleeping for {sleep_time} seconds\")\n        time.sleep(sleep_time)\n        result = call_func()\n        attempts += 1\n    return result\n</code></pre>"}]}